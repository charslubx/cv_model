"""
DeepFashionå±æ€§è¯†åˆ«å•ç‹¬è®­ç»ƒè„šæœ¬
ç”¨äºè®­ç»ƒFullAdaGATæ¨¡å‹è¿›è¡Œæœè£…å±æ€§è¯†åˆ«
"""

import os
import logging
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import argparse

from ablation_models import FullAdaGAT
from training import DeepFashionDataset, DeepFashionTrainer

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='DeepFashionå±æ€§è¯†åˆ«è®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=30, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--save_dir', type=str, default='deepfashion_checkpoints', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--lambda_threshold', type=float, default=0.5, help='FullAdaGATçš„lambdaé˜ˆå€¼')
    
    args = parser.parse_args()
    
    logger.info("=" * 80)
    logger.info("DeepFashionå±æ€§è¯†åˆ«è®­ç»ƒ")
    logger.info("=" * 80)
    
    # æ•°æ®é›†è·¯å¾„
    DEEPFASHION_ROOT = "/home/cv_model/DeepFashion"
    CATEGORY_ROOT = os.path.join(DEEPFASHION_ROOT, "Category and Attribute Prediction Benchmark")
    ANNO_DIR = os.path.join(CATEGORY_ROOT, "Anno_fine")
    IMG_DIR = os.path.join(CATEGORY_ROOT, "Img")
    
    # æ£€æŸ¥DeepFashionæ•°æ®é›†
    if not os.path.exists(DEEPFASHION_ROOT):
        logger.error(f"DeepFashionæ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {DEEPFASHION_ROOT}")
        exit(1)
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        os.path.join(ANNO_DIR, "train.txt"),
        os.path.join(ANNO_DIR, "train_attr.txt"),
        os.path.join(ANNO_DIR, "val.txt"),
        os.path.join(ANNO_DIR, "val_attr.txt")
    ]
    
    if not all(os.path.exists(f) for f in required_files):
        logger.error("DeepFashionæ•°æ®é›†æ–‡ä»¶ä¸å®Œæ•´ï¼")
        logger.error("ç¼ºå¤±çš„æ–‡ä»¶:")
        for f in required_files:
            if not os.path.exists(f):
                logger.error(f"  - {f}")
        exit(1)
    
    logger.info("âœ“ DeepFashionæ•°æ®é›†æ£€æŸ¥é€šè¿‡")
    
    # æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½DeepFashionæ•°æ®é›†
    logger.info("åŠ è½½DeepFashionæ•°æ®é›†...")
    TRAIN_IMG_LIST = os.path.join(ANNO_DIR, "train.txt")
    TRAIN_ATTR_FILE = os.path.join(ANNO_DIR, "train_attr.txt")
    VAL_IMG_LIST = os.path.join(ANNO_DIR, "val.txt")
    VAL_ATTR_FILE = os.path.join(ANNO_DIR, "val_attr.txt")
    
    train_dataset = DeepFashionDataset(
        img_list_file=TRAIN_IMG_LIST,
        attr_file=TRAIN_ATTR_FILE,
        image_dir=IMG_DIR,
        transform=train_transform
    )
    
    val_dataset = DeepFashionDataset(
        img_list_file=VAL_IMG_LIST,
        attr_file=VAL_ATTR_FILE,
        image_dir=IMG_DIR,
        transform=val_transform
    )
    
    logger.info(f"DeepFashionåŠ è½½æˆåŠŸ: è®­ç»ƒé›†{len(train_dataset)}, éªŒè¯é›†{len(val_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        drop_last=False
    )
    
    logger.info(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ: è®­ç»ƒæ‰¹æ¬¡{len(train_loader)}, éªŒè¯æ‰¹æ¬¡{len(val_loader)}")
    
    # åˆ›å»ºFullAdaGATæ¨¡å‹
    logger.info("åˆ›å»ºFullAdaGATæ¨¡å‹...")
    model = FullAdaGAT(
        num_classes=26,  # DeepFashionå±æ€§æ•°é‡
        lambda_threshold=args.lambda_threshold
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    logger.info(f"Lambdaé˜ˆå€¼: {args.lambda_threshold}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = DeepFashionTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        learning_rate=args.lr
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("=" * 80)
    logger.info(f"å¼€å§‹è®­ç»ƒ...")
    logger.info(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"å­¦ä¹ ç‡: {args.lr}")
    logger.info(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    logger.info("=" * 80)
    
    try:
        trainer.train(epochs=args.epochs, save_dir=args.save_dir)
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ DeepFashionè®­ç»ƒæˆåŠŸå®Œæˆ!")
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_dir}/best_model.pth")
        logger.info(f"æœ€ä½³F1åˆ†æ•°: {trainer.best_f1:.4f}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error("=" * 80)
        logger.error("âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯!")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
        logger.error("=" * 80)
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()

